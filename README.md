# CDR Relation Extraction with BioBERT

This repository contains scripts to fine-tune BioBERT on the BC5-CDR dataset for chemicalâ€“disease relation extraction. Three variants are provided:

1. **baseline\_bc5\_finetune.py** â€“ Simple prompt-based input with `[CHEM]`/`[DIS]` tokens.
2. **bc5\_entity.py** â€“ Entity-marker variant inserting `[E1]â€¦[/E1]` and `[E2]â€¦[/E2]` around chemical and disease spans.
3. **bc5\_entity\_with\_reg\_cosine.py** â€“ Enhanced variant adding dropout, label smoothing, and cosine LR scheduling.

---

## ğŸ“ Repository Structure

```
BioBERT-Fine-Tuning/
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ his_logs
â”‚   â”œâ”€â”€ log_baseline.log
â”‚   â”œâ”€â”€ log_entity.log
â”‚   â””â”€â”€ log_entity_cosine.log
â”œâ”€â”€ model
â”‚   â””â”€â”€ dmis-lab
â”‚       â””â”€â”€ biobert-v1.1
â”œâ”€â”€ process_data
â”‚   â”œâ”€â”€ dev.json
â”‚   â”œâ”€â”€ test.json
â”‚   â””â”€â”€ train.json
â””â”€â”€ script
    â”œâ”€â”€ baseline_bc5_finetune.py
    â”œâ”€â”€ bc5_entity.py
    â”œâ”€â”€ bc5_entity_with_reg_cosine.py 
    â””â”€â”€ preprocess.py       
```

## ğŸš€ Requirements

* Python 3.8+
* PyTorch 1.12+
* Transformers 4.21+
* Datasets 2.x
* scikit-learn

Install via:

```bash
pip install torch torchvision transformers datasets scikit-learn
```

## ğŸ”§ Preparation
- I provide my preprocessed data for simplicity https://huggingface.co/datasets/n0v1cee/BC5-CDR-Balanced/tree/main. Or if you would like to download and do preprocessing from beginning, try:
- Download BC5-CDR annotations. https://github.com/JHnlp/BioCreative-V-CDR-Corpus
- Run `script/preprocess.py`, remember to substitute your data path in this script, it should
  - Convert to JSON format with fields: `text`, `chemical`, `disease`, `label`. 
  - Place files under `processed_data/` as `train.json`, `dev.json`, `test.json`. 
- Download model under `model/dmis-lab/` as `biobert-v1.1` https://huggingface.co/dmis-lab/biobert-v1.1/tree/main


## ğŸ“‹ Usage

  We ran training on 4 4090 GPUs using `torchrun`:

* **Baseline**

  ```bash
  torchrun --nproc_per_node=4 baseline_bc5_finetune.py
  ```

* **Entity Marker**
  ```bash
  torchrun --nproc_per_node=4 bc5_entity.py
  ```

* **Enhanced (reg + cosine)**

  ```bash
  torchrun --nproc_per_node=4 bc5_entity_with_reg_cosine.py
  ```

Scripts will:
- Load `dmis-lab/biobert-v1.1`.  
- Fine-tune on train set, evaluate on dev each epoch.  
- Save best checkpoint by `eval_f1`.  
- Finally evaluate on test set and print metrics.

## âš™ï¸ Configuration
Edit script-level parameters at the top of each `.py` file:
- `model_name` â€“ pretrained model identifier.  
- Data paths (`processed_data/train.json`, etc.).  
- Hyperparameters (`learning_rate`, `batch_size`, `num_train_epochs`, etc.).

## ğŸ“Š Results
After training, look under `./result/...` for:
- `checkpoint-*` folders with saved models.  
- `log/` directory with TensorBoard logs.  

Run:
```bash
python -c "from transformers import Trainer; print('See results printed at end of script.')"
````

---

## âœ¨ Notes
* Check `metric_for_best_model='eval_f1'` matches your `compute_metrics` keys.
* Use `model.eval()` and reload best checkpoint before final test evaluation.

## ğŸ“œ License

MIT
